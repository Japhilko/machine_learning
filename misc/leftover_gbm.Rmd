---
title: "leftover gbm"
author: "Jan-Philipp Kolb"
date: "4 6 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## The necessary packages 

```{r necpackagesgbm}
library(rsample)      # data splitting 
library(gbm)          # basic implementation
library(xgboost)      # a faster implementation of gbm
library(caret)        # aggregator package - machine learning
library(pdp)          # model visualization
library(ggplot2)      # model visualization
library(lime)         # model visualization
```

```{r,eval=F,echo=F}
install.packages("gbm")
install.packages("lime")
install.packages("vtreat")
install.packages("gbm")
install.packages("lime")
install.packages("vtreat")
```


## The dataset

- Again, we use the Ames housing dataset 

```{r,eval=F}
ames_data <- AmesHousing::make_ames()
```

```{r,eval=F,echo=F}
save(ames_data,file="../data/ames_data.RData")
```

```{r,echo=F}
load("../data/ames_data.RData")
```


```{r}
set.seed(123)
ames_split <- initial_split(ames_data,prop=.7)
ames_train <- training(ames_split)
ames_test  <- testing(ames_split)
```


## Package implementation

The most popular implementations of GBM in R:

### gbm

The original R implementation of GBMs

### xgboost

A fast and efficient gradient boosting framework (C++ backend).

### h2o

A powerful java-based interface that provides parallel distributed algorithms and efficient productionalization.

## The R-package `gbm`

- The `gbm` R package is an implementation of extensions to Freund and Schapire’s [**AdaBoost**](https://en.wikipedia.org/wiki/AdaBoost) algorithm and [**Friedman’s gradient boosting machine**](https://www.frontiersin.org/articles/10.3389/fnbot.2013.00021/full). 

<!--
This is the original R implementation of GBM. 

https://www.slideshare.net/mark_landry/gbm-package-in-r
-->

![](figure/package_gbm.PNG)

## Basic implementation - training function

- Two primary training functions are available: `gbm::gbm` and `gbm::gbm.fit`. 
- `gbm::gbm` uses the formula interface to specify the model 
- `gbm::gbm.fit` requires the separated x and y matrices (more efficient with many variables). 

<!--
When working with many variables it is more efficient to use the matrix rather than formula interface.
-->


- The default settings in `gbm` include a learning rate (shrinkage) of 0.001.
- This is a very small learning rate and typically requires a large number of trees to find the minimum MSE. 
- `gbm` uses the default number of 100 trees, which is rarely sufficient. 
- The default depth of each tree (`interaction.depth`) is 1, which means we are ensembling a bunch of stumps. 
- We will use `cv.folds` to perform a 5 fold cross validation. 
- The model takes about 90 seconds to run and the results show that the MSE loss function is minimized with 10000 trees.

## Train a GBM model

<!--
folgendes dauert auch wieder länger

Either a character string specifying the name of the distribution to use or a list with a component name specifying the distribution and any additional parameters needed. If not specified, gbm will try to guess: if the response has only 2 unique values, bernoulli is assumed; otherwise, if the response is a factor, multinomial is assumed; otherwise, if the response has class "Surv", coxph is assumed; otherwise, gaussian is assumed.

Currently available options are "gaussian" (squared error), "laplace" (absolute loss), "tdist" (t-distribution loss), "bernoulli" (logistic regression for 0-1 outcomes), "huberized" (huberized hinge loss for 0-1 outcomes), classes), "adaboost" (the AdaBoost exponential loss for 0-1 outcomes), "poisson" (count outcomes), "coxph" (right censored observations), "quantile", or "pairwise" (ranking measure using the LambdaMart algorithm).

If quantile regression is specified, distribution must be a list of the form list(name = "quantile", alpha = 0.25) where alpha is the quantile to estimate. The current version's quantile regression method does not handle non-constant weights and will stop.

If "tdist" is specified, the default degrees of freedom is 4 and this can be controlled by specifying distribution = list(name = "tdist", df = DF) where DF is your chosen degrees of freedom.

If "pairwise" regression is specified, distribution must be a list of the form list(name="pairwise",group=...,metric=...,max.rank=...) (metric and max.rank are optional, see below). group is a character vector with the column names of data that jointly indicate the group an instance belongs to (typically a query in Information Retrieval applications). For training, only pairs of instances from the same group and with different target labels can be considered. metric is the IR measure to use, one of

list("conc")
Fraction of concordant pairs; for binary labels, this is equivalent to the Area under the ROC Curve

:
Fraction of concordant pairs; for binary labels, this is equivalent to the Area under the ROC Curve

list("mrr")
Mean reciprocal rank of the highest-ranked positive instance

:
Mean reciprocal rank of the highest-ranked positive instance

list("map")
Mean average precision, a generalization of mrr to multiple positive instances

:
Mean average precision, a generalization of mrr to multiple positive instances

list("ndcg:")
Normalized discounted cumulative gain. The score is the weighted sum (DCG) of the user-supplied target values, weighted by log(rank+1), and normalized to the maximum achievable value. This is the default if the user did not specify a metric.

ndcg and conc allow arbitrary target values, while binary targets 0,1 are expected for map and mrr. For ndcg and mrr, a cut-off can be chosen using a positive integer parameter max.rank. If left unspecified, all ranks are taken into account.

Note that splitting of instances into training and validation sets follows group boundaries and therefore only approximates the specified train.fraction ratio (the same applies to cross-validation folds). Internally, queries are randomly shuffled before training, to avoid bias.

Weights can be used in conjunction with pairwise metrics, however it is assumed that they are constant for instances from the same group.

For details and background on the algorithm, see e.g. Burges (2010).
-->

- `distribution` - depends on the response (e.g. bernoulli for binomial)
- `gaussian` is the defaulf value

```{r,eval=timeconsuming}
set.seed(123)
gbm.fit <- gbm(formula = Sale_Price ~ .,distribution="gaussian",
  data = ames_train,n.trees = 10000,interaction.depth = 1,
  shrinkage = 0.001,cv.folds = 5,n.cores = NULL,verbose = FALSE)  
```

```{r,echo=F,eval=F}
# das dauert recht lange
save(gbm.fit,file="../data/ml_gbm_fit.RData")
```

```{r,echo=F}
load("../data/ml_gbm_fit.RData")
```


```{r}
print(gbm.fit) # print results
```

## Exercise

- Take some time to dig around in the `gbm.fit` object to get comfortable with its components. 


## The output object...

- ... is a list containing several modelling and results information. 
- We can access this information with regular indexing; 
- The minimum CV RMSE is 29133 (this means on average our model is about $29,133 off from the actual sales price) but the plot also illustrates that the CV error is still decreasing at 10,000 trees.

### Get MSE

```{r}
sqrt(min(gbm.fit$cv.error))
```

## Plot loss function as a result of n trees added to the ensemble

```{r}
gbm.perf(gbm.fit, method = "cv")
```

<!--
In this case, the small learning rate is resulting in very small incremental improvements which means many trees are required. In fact, for the default learning rate and tree depth settings it takes 39,906 trees for the CV error to minimize (~ 5 minutes of run time)!
-->

## Tuning GBMs
<!--
However, rarely do the default settings suffice. We could tune parameters one at a time to see how the results change. 
-->
- The learning rate is increased to take larger steps down the gradient descent, 
- The number of trees is reduced (since we reduced the learning rate), 
and increase the depth of each tree.

<!--
using a single split to 3 splits.
This model takes about 90 seconds to run and achieves a significantly lower RMSE than our initial model with only 1,260 trees.
-->

```{r,eval=F}
set.seed(123)
gbm.fit2 <- gbm(formula = Sale_Price ~ .,
  distribution = "gaussian",data = ames_train,
  n.trees = 5000,interaction.depth = 3,shrinkage = 0.1,
  cv.folds = 5,n.cores = NULL,verbose = FALSE)  
```

```{r,eval=F,echo=F}
save(gbm.fit2,file="../data/ml_gbm_fit2.RData")
```

```{r,echo=F}
load("../data/ml_gbm_fit2.RData")
```


```{r}
# find index for n trees with minimum CV error
min_MSE <- which.min(gbm.fit2$cv.error)
# get MSE and compute RMSE
sqrt(gbm.fit2$cv.error[min_MSE])
```

## plot loss function as a result of n trees added to the ensemble

### Assess the GBM performance:

```{r}
gbm.perf(gbm.fit2, method = "cv")
```


## Grid search

<!--
However, a better option than manually tweaking hyperparameters one at a time is to perform a grid search which iterates over every combination of hyperparameter values and allows us to assess which combination tends to perform well. To perform a manual grid search, first we want to construct our grid of hyperparameter combinations. We’re going to search across 81 models with varying learning rates and tree depth. I also vary the minimum number of observations allowed in the trees terminal nodes (n.minobsinnode) and introduce stochastic gradient descent by allowing bag.fraction < 1.
-->

- `n.minobsinnode` is the minimum number of observations allowed in the trees (nr. for terminal nodes is varied)

```{r}
hyper_grid <- expand.grid(
  shrinkage = c(.01, .1, .3),
  interaction.depth = c(1, 3, 5),
  n.minobsinnode = c(5, 10, 15),
  bag.fraction = c(.65, .8, 1), 
  optimal_trees = 0,# a place to dump results
  min_RMSE = 0                     
)

# total number of combinations
nrow(hyper_grid)
```


## Randomize data

- `train.fraction` use the first XX% of the data so its important to randomize the rows in case there is any logic ordering (i.e. ordered by neighborhood).

```{r}
random_index <- sample(1:nrow(ames_train), nrow(ames_train))
random_ames_train <- ames_train[random_index, ]
```


## Grid search - loop over hyperparameter grid

```{r,eval=F}
for(i in 1:nrow(hyper_grid)) {
  set.seed(123)
  gbm.tune <- gbm(
    formula = Sale_Price ~ .,distribution = "gaussian",
    data = random_ames_train,n.trees = 5000,
    interaction.depth = hyper_grid$interaction.depth[i],
    shrinkage = hyper_grid$shrinkage[i],
    n.minobsinnode = hyper_grid$n.minobsinnode[i],
    bag.fraction = hyper_grid$bag.fraction[i],
    train.fraction = .75,n.cores = NULL,verbose = FALSE
  )
  # add min training error and trees to grid
  hyper_grid$optimal_trees[i] <- which.min(gbm.tune$valid.error)
  hyper_grid$min_RMSE[i] <- sqrt(min(gbm.tune$valid.error))
}
```


```{r,eval=F,echo=F}
save(gbm.tune,hyper_grid,file="../data/ml_gbm_tune.RData")
```

```{r,echo=F}
load("../data/ml_gbm_tune.RData")
```


## The top 10 values
  
```{r,eval=F}
hyper_grid %>% 
  dplyr::arrange(min_RMSE) %>%
  head(10)
```

```{r,echo=F,eval=F}
top10 <- hyper_grid %>% 
  dplyr::arrange(min_RMSE) %>%
  head(10)
```

![](figure/top10gbms.PNG)

## Loop through hyperparameter combinations

- We loop through each hyperparameter combination (5,000 trees). 
-  To speed up the tuning process, instead of performing 5-fold CV we train on 75% of the training observations and evaluate performance on the remaining 25%. 
<!--
Important note:
-->

<!--
After about 30 minutes of training time our grid search ends and we see a few important results pop out. 
-->
- The top model has better performance than our previously fitted model, with a RMSE nearly $3,000 and lower. 

### A look at the top 10 models:
- None of the top models used a learning rate of 0.3; small incremental steps down the gradient descent work best,
- None of the top models used stumps (`interaction.depth = 1`); there are likely some important interactions that the deeper trees are able to capture.
- Adding a stochastic component with `bag.fraction` < 1 seems to help; there may be some local minimas in our loss function gradient,

<!--
- none of the top models used `n.minobsinnode = 15`; the smaller nodes may allow us to capture pockets of unique feature-price point instances,


- in a few instances we appear to use nearly all 5,000 trees; 

maybe we should increase this parameter in our next search?
-->


## Refine the search - adjust the grid

```{r}
# modify hyperparameter grid
hyper_grid <- expand.grid(
  shrinkage = c(.01, .05, .1),
  interaction.depth = c(3, 5, 7),
  n.minobsinnode = c(5, 7, 10),
  bag.fraction = c(.65, .8, 1), 
  optimal_trees = 0,# a place to dump results
  min_RMSE = 0# a place to dump results
)

# total number of combinations
nrow(hyper_grid)
```

## The final model

```{r}
set.seed(123)
# train GBM model
gbm.fit.final <- gbm(formula = Sale_Price ~ .,
  distribution = "gaussian",data = ames_train,
  n.trees = 483,interaction.depth = 5,
  shrinkage = 0.1,n.minobsinnode = 5,
  bag.fraction = .65,train.fraction = 1,
  n.cores = NULL, # will use all cores by default
  verbose = FALSE)  
```


## Visualizing - [Variable importance](https://topepo.github.io/caret/variable-importance.html)

- `cBars` allows you to adjust the number of variables to show

```{r}
summary(gbm.fit.final,cBars = 10,
  # also can use permutation.test.gbm
  method = relative.influence,las = 2)
```

## Variable importance

![](figure/gbmtopmodelsvars.PNG)

<!--
par(mar = c(5, 8, 1, 1))


## The `vip` package

```{r,eval=F}
devtools::install_github("koalaverse/vip")
```

```{r,eval=F}
vip::vip(gbm.fit.final)
```
-->


## Partial dependence plots

- [**PDPs**](https://christophm.github.io/interpretable-ml-book/pdp.html) show the marginal effect one or two features have on the predicted outcome. 

<!--
plot the change in the average predicted value as specified feature(s) vary over their marginal distribution. 
-->
- The following PDP plot displays the average change in predicted sales price as we vary `Gr_Liv_Area` while holding all other variables constant. 
<!--
- This is done by holding all variables constant for each observation in our training data set but then apply the unique values of Gr_Liv_Area for each observation. 
-->
- We then average the sale price across all the observations. 
- This PDP illustrates how the predicted sales price increases as the square footage of the ground floor in a house increases.

### Partial dependence plot - `Gr_Liv_Area`

```{r,eval=F}
gbm.fit.final %>% partial(pred.var = "Gr_Liv_Area", 
          n.trees = gbm.fit.final$n.trees, 
          grid.resolution = 100) %>%
  autoplot(rug = TRUE, train = ames_train) +
  scale_y_continuous(labels = scales::dollar)
```

## Partial dependence plot

```{r,echo=F}
gbm.fit.final %>% partial(pred.var = "Gr_Liv_Area", 
          n.trees = gbm.fit.final$n.trees, 
          grid.resolution = 100) %>%
  autoplot(rug = TRUE, train = ames_train) +
  scale_y_continuous(labels = scales::dollar)
```


## [Individual Conditional Expectation (ICE) curves ... ](https://christophm.github.io/interpretable-ml-book/ice.html)

- ... are an extension of PDP plots but 
<!--
, rather than plot the average marginal effect on the response variable, we plot 
-->
the change in the predicted response variable is plotted as we vary each predictor variable. 
<!--
- Below shows the regular ICE curve plot (left) and the centered ICE curves (right). 
-->
- When the curves have a wide range of intercepts and are consequently “stacked” on each other, heterogeneity in the response variable values due to marginal changes in the predictor variable of interest can be difficult to discern. 
- The centered ICE can help draw these inferences out and can highlight any strong heterogeneity in our results. 
- The results show that most observations follow a common trend as `Gr_Liv_Area` increases; 
- the centered ICE plot highlights a few observations that deviate from the common trend.

## Non centered ICE curve

```{r,eval=F}
ice1 <- gbm.fit.final %>%
  partial(
    pred.var = "Gr_Liv_Area", 
    n.trees = gbm.fit.final$n.trees, 
    grid.resolution = 100,
    ice = TRUE
    ) %>%
  autoplot(rug = TRUE, train = ames_train, alpha = .1) +
  ggtitle("Non-centered") +
  scale_y_continuous(labels = scales::dollar)
```

## Centered ICE curve

```{r,eval=F}
ice2 <- gbm.fit.final %>%
  partial(
    pred.var = "Gr_Liv_Area", 
    n.trees = gbm.fit.final$n.trees, 
    grid.resolution = 100,
    ice = TRUE
    ) %>%
  autoplot(rug = TRUE, train = ames_train, alpha = .1, 
           center = TRUE) +  ggtitle("Centered") +
  scale_y_continuous(labels = scales::dollar)
```

```{r,echo=F,eval=F}
ice2 <- gbm.fit.final %>%
  partial(
    pred.var = "Gr_Liv_Area", 
    n.trees = gbm.fit.final$n.trees, 
    grid.resolution = 100,
    ice = TRUE
    ) %>%
  autoplot(rug = TRUE, train = ames_train, alpha = .1, center = TRUE) +
  ggtitle("Centered") +
  scale_y_continuous(labels = scales::dollar)
```


## Non centered and centered ice curve

```{r,eval=F}
gridExtra::grid.arrange(ice1, ice2, nrow = 1)
```

```{r,eval=F,echo=F}
png("figure/ml_ice_curves.png")
  gridExtra::grid.arrange(ice1, ice2, nrow = 1)
dev.off()
```

![](figure/ml_ice_curves.png){height=80%}


## [Local Interpretable Model-Agnostic Explanations – (LIME)](https://homes.cs.washington.edu/~marcotcr/blog/lime/)

- [**LIME**](http://uc-r.github.io/lime) is a newer procedure for understanding why a prediction resulted in a given value for a single observation. 
- To use the `lime` package on a `gbm` model we need to define model type and prediction methods.

```{r}
model_type.gbm <- function(x, ...) {
  return("regression")
}

predict_model.gbm <- function(x, newdata, ...) {
  pred <- predict(x, newdata, n.trees = x$n.trees)
  return(as.data.frame(pred))
}
```

## Applying LIME

<!--
- We can now apply to our two observations. 
-->
- The results show the predicted value (Case 1: 118K Dollar, Case 2: 161K Dollar), local model fit (both are relatively poor), and the most influential variables driving the predicted value for each observation.

```{r}
# get a few observations to perform local interpretation on
local_obs <- ames_test[1:2, ]

# apply LIME
explainer <- lime(ames_train, gbm.fit.final)
explanation <- explain(local_obs, explainer, n_features = 5)
```

## LIME plot

```{r,eval=F}
plot_features(explanation)
```

```{r,eval=F,echo=F}
png("figure/limeplot.png")
plot_features(explanation)
dev.off()
```

![](figure/limeplot.png)


## Predicting

- If you have decided on a final model you'll likely want to use the model to predict on new observations. 
- Like most models, we simply use the predict function; we also need to supply the number of trees to use (see `?predict.gbm` for details). 
- The RMSE for the test set is very close to the RMSE we obtained on our best gbm model.

```{r}
# predict values for test data
pred <- predict(gbm.fit.final, n.trees = gbm.fit.final$n.trees, 
                ames_test)

# results
caret::RMSE(pred, ames_test$Sale_Price)
```

## `xgboost`

- The `xgboost` R package provides an R API to “Extreme Gradient Boosting”, which is an efficient implementation of gradient boosting framework (approx. 10x faster than gbm). 
- The xgboost/demo repository provides a wealth of information. 
<!--
You can also find a fairly comprehensive parameter tuning guide here. 

https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/
-->
<!--
- The `xgboost` package has been quite popular and successful on Kaggle for data mining competitions.
-->

### Features include:

- Provides built-in k-fold cross-validation
 -Stochastic GBM with column and row sampling (per split and per tree) for better generalization.
- Includes efficient linear model solver and tree learning algorithms.
- Parallel computation on a single machine.
- Supports various objective functions, including regression, classification and ranking.
- The package is made to be extensible, so that users are also allowed to define their own objectives easily.
- Apache 2.0 License.


## Basic implementation

- `XGBoost` only works with matrices that contain all numeric variables; consequently, we need to hot encode our data. There are different ways to do this in R (i.e. `Matrix::sparse.model.matrix`, `caret::dummyVars`) but here we will use the `vtreat` package. 
- `vtreat` is a robust package for data prep and helps to eliminate problems caused by missing values, novel categorical levels that appear in future data sets that were not in the training data, etc. `vtreat` is not very intuitive. 


<!--
I will not explain the functionalities but you can find more information here, here, and here.

https://arxiv.org/abs/1611.09477
https://www.r-bloggers.com/a-demonstration-of-vtreat-data-preparation/
https://github.com/WinVector/vtreat
-->

## Application of `vtreat` to one-hot encode the training and testing data sets.


```{r}
# variable names
features <- setdiff(names(ames_train), "Sale_Price")
# Create the treatment plan from the training data
treatplan <- vtreat::designTreatmentsZ(ames_train, features, 
                                       verbose = FALSE)
```

```{r}
# Get the "clean" variable names from the scoreFrame
new_vars <- treatplan %>%
  magrittr::use_series(scoreFrame) %>%        
  dplyr::filter(code %in% c("clean", "lev")) %>% 
  magrittr::use_series(varName)     

# Prepare the training data
features_train <- vtreat::prepare(treatplan, ames_train, 
                varRestriction = new_vars) %>% as.matrix()
response_train <- ames_train$Sale_Price
```

## Prepare the test data

```{r}
features_test <- vtreat::prepare(treatplan, ames_test, 
                varRestriction = new_vars) %>% as.matrix()
response_test <- ames_test$Sale_Price
```

### dimensions of one-hot encoded data

```{r}
dim(features_train)
dim(features_test)
```



## `xgboost` - training functions

- `xgboost` provides different training functions (i.e. `xgb.train` which is just a wrapper for `xgboost`). 
- To train an XGBoost we typically want to use `xgb.cv`, which incorporates cross-validation. The following trains a basic 5-fold cross validated XGBoost model with 1,000 trees. There are many parameters available in `xgb.cv` but the ones used in this tutorial include the following default values:

- learning rate ($\eta$): 0.3
- tree depth (`max_depth`): 6
- minimum node size (`min_child_weight`): 1
- percent of training data to sample for each tree (subsample –> equivalent to gbm’s `bag.fraction`): 100%

## Extreme gradient boosting for regression models

```{r,eval=timeconsuming}
set.seed(123)
xgb.fit1 <- xgb.cv(
  data = features_train,
  label = response_train,
  nrounds = 1000,  nfold = 5,
  objective = "reg:linear",  # for regression models
  verbose = 0               # silent,
)
```

```{r,echo=F,eval=F}
save(xgb.fit1,file="../data/ml_rf_xgb.fit1.RData")
```

```{r,echo=F}
load("../data/ml_rf_xgb.fit1.RData")
```


## get number of trees that minimize error

- The `xgb.fit1` object contains lots of good information. 
- In particular we can assess the `xgb.fit1$evaluation_log` to identify the minimum RMSE and the optimal number of trees for both the training data and the cross-validated error. 
- The training error continues to decrease to 924 trees where the RMSE nearly reaches zero; 
- The cross validated error reaches a minimum RMSE of 27,337 with only 60 trees.

```{r}
xgb.fit1$evaluation_log %>%
  dplyr::summarise(
  ntrees.train=which(train_rmse_mean==min(train_rmse_mean))[1],
  rmse.train= min(train_rmse_mean),
  ntrees.test=which(test_rmse_mean==min(test_rmse_mean))[1],
  rmse.test   = min(test_rmse_mean)
)
```




## Plot error vs number trees

```{r}
ggplot(xgb.fit1$evaluation_log) +
  geom_line(aes(iter, train_rmse_mean), color = "red") +
  geom_line(aes(iter, test_rmse_mean), color = "blue")
```


## Early stopping

- A nice feature provided by `xgb.cv` is early stopping. 
- This allows us to tell the function to stop running if the cross validated error does not improve for n continuous trees. 
- E.g., the above model could be re-run with the following where we tell it stop if we see no improvement for 10 consecutive trees. This feature will help us speed up the tuning process. 

```{r}
set.seed(123)
xgb.fit2 <- xgb.cv(data = features_train,
  label = response_train,
  nrounds = 1000,  nfold = 5,
  objective = "reg:linear",  # for regression models
  verbose = 0,               # silent,
  # stop if no improvement for 10 consecutive trees
  early_stopping_rounds = 10)
```

## plot error vs number trees

```{r, fig.height = 3, fig.width = 6, fig.align = "center"}
ggplot(xgb.fit2$evaluation_log) +
  geom_line(aes(iter, train_rmse_mean), color = "red") +
  geom_line(aes(iter, test_rmse_mean), color = "blue")
```

```{r,eval=F,echo=F}
ggplot(xgb.fit2$evaluation_log) +
  geom_line(aes(iter, train_rmse_mean), color = "red") +
  geom_line(aes(iter, test_rmse_mean), color = "blue")
```


## Tuning

- To tune the XGBoost model we pass parameters as a list object to the params argument. The most common parameters include:

- `eta:controls`-  the learning rate
- `max_depth`: tree depth
- `min_child_weight`: minimum number of observations required in each terminal node
- `subsample`: percent of training data to sample for each tree
- `colsample_bytrees`: percent of columns to sample from for each tree
- E.g. to specify specific values for these parameters we would extend the above model with the following parameters.

## create parameter list

```{r}
  params <- list(
    eta = .1,
    max_depth = 5,
    min_child_weight = 2,
    subsample = .8,
    colsample_bytree = .9
  )
```

## To perform a large search grid,...

- we can follow the same procedure we did with `gbm`. 
- We create our hyperparameter search grid along with columns to dump our results in. 
- Here, we have a pretty large search grid consisting of 576 different hyperparameter combinations to model.

```{r}
# create hyperparameter grid
hyper_grid <- expand.grid(
  eta = c(.01, .05, .1, .3),
  max_depth = c(1, 3, 5, 7),
  min_child_weight = c(1, 3, 5, 7),
  subsample = c(.65, .8, 1), 
  colsample_bytree = c(.8, .9, 1),
  optimal_trees = 0,# a place to dump results
  min_RMSE = 0# a place to dump results
)

nrow(hyper_grid)
```

## train model

```{r}
set.seed(123)
xgb.fit3 <- xgb.cv(
  params = params,
  data = features_train,
  label = response_train,
  nrounds = 1000,
  nfold = 5,
  objective = "reg:linear",  # for regression models
  verbose = 0,               # silent,
  # stop if no improvement for 10 consecutive trees
  early_stopping_rounds = 10 
)
```

```{r,echo=F,eval=F}
save(xgb.fit3,file="../data/ml_rf_xgb.fit3.RData")
```

```{r,echo=F}
load("../data/ml_rf_xgb.fit3.RData")
```


## assess results

```{r}
xgb.fit3$evaluation_log %>%
  dplyr::summarise(
    ntrees.train=which(train_rmse_mean==min(train_rmse_mean))[1],
    rmse.train= min(train_rmse_mean),
    ntrees.test= which(test_rmse_mean==min(test_rmse_mean))[1],
    rmse.test= min(test_rmse_mean)
  )
```


## Loop through a `XGBoost` model

- We apply the same in the loop and apply a `XGBoost` model for each hyperparameter combination and dump the results in the `hyper_grid` data frame.

### Important note: 

- If you plan to run this code be prepared to run it before going out to eat or going to bed as it the full search grid took 6 hours to run!

## Grid search 

```{r,eval=F}
for(i in 1:nrow(hyper_grid)) {
  params <- list(# create parameter list
    eta = hyper_grid$eta[i],max_depth = hyper_grid$max_depth[i],
    min_child_weight = hyper_grid$min_child_weight[i],
    subsample = hyper_grid$subsample[i],
    colsample_bytree = hyper_grid$colsample_bytree[i])
  set.seed(123)
  xgb.tune <- xgb.cv(params = params,data = features_train,
  label = response_train,nrounds=5000,nfold=5,objective = "reg:linear",
  #stop if no improvement for 10 consecutive trees
    verbose = 0,early_stopping_rounds = 10 )
  # add min training error and trees to grid
  hyper_grid$optimal_trees[i]<-which.min(
    xgb.tune$evaluation_log$test_rmse_mean)
  hyper_grid$min_RMSE[i] <- min(
    xgb.tune$evaluation_log$test_rmse_mean)
}
```

```{r,eval=F,echo=F}
save(xgb.tune,hyper_grid,file="../data/ml_gbm_reg_linear.RData")
```

```{r,echo=F,eval=F}
load("../data/ml_gbm_reg_linear.RData")
```


## Result - top 10 models

```{r}
hyper_grid %>%
  dplyr::arrange(min_RMSE) %>%
  head(10)
```


## The top model

- After assessing the results you would likely perform a few more grid searches to hone in on the parameters that appear to influence the model the most. 
<!--
- Here is a link to a great blog post that discusses a strategic approach to tuning with `xgboost`. 
-->
- We’ll just assume the top model in the above search is the globally optimal model. Once you’ve found the optimal model, we can fit our final model with `xgb.train`.

```{r}
# parameter list
params <- list(
  eta = 0.01,
  max_depth = 5,
  min_child_weight = 5,
  subsample = 0.65,
  colsample_bytree = 1
)
```

## Train final model

```{r}
xgb.fit.final <- xgboost(
  params = params,
  data = features_train,
  label = response_train,
  nrounds = 1576,
  objective = "reg:linear",
  verbose = 0
)
```


## [**Input types for `xgboost`**](https://cran.r-project.org/web/packages/xgboost/vignettes/xgboostPresentation.html#manipulating-xgb.dmatrix)

Input type: `xgboost` takes several types of input data:

- Dense Matrix: R’s dense matrix, i.e. matrix ;
-    Sparse Matrix: R’s sparse matrix, i.e. Matrix::dgCMatrix ;
-    Data File: local data files ;
-    `xgb.DMatrix`: its own class (recommended).

### get information

- We get information on an `xgb.DMatrix` object with `getinfo`




## Visualizing

### Variable importance

`xgboost` provides built-in variable importance plotting. First, you need to create the importance matrix with xgb.importance and then feed this matrix into xgb.plot.importance. There are 3 variable importance measure:

- Gain: the relative contribution of the corresponding feature to the model calculated by taking each feature’s contribution for each tree in the model. This is synonymous with gbm’s relative.influence.
- Cover: the relative number of observations related to this feature. For example, if you have 100 observations, 4 features and 3 trees, and suppose feature1 is used to decide the leaf node for 10, 5, and 2 observations in tree1, tree2 and tree3 respectively; then the metric will count cover for this feature as 10+5+2 = 17 observations. This will be calculated for all the 4 features and the cover will be 17 expressed as a percentage for all features’ cover metrics.

## create importance matrix

- Frequency: the percentage representing the relative number of times a particular feature occurs in the trees of the model. In the above example, if feature1 occurred in 2 splits, 1 split and 3 splits in each of tree1, tree2 and tree3; then the weightage for feature1 will be 2+1+3 = 6. The frequency for feature1 is calculated as its percentage weight over weights of all features.

```{r}
importance_matrix <- xgb.importance(model = xgb.fit.final)
```

### variable importance plot

```{r,eval=F}
xgb.plot.importance(importance_matrix, top_n = 10, measure = "Gain")
```

## Variable importance plot

```{r,echo=F}
xgb.plot.importance(importance_matrix, top_n = 10, measure = "Gain")
```


<!--
## Partial dependence plots

PDP and ICE plots work similarly to how we implemented them with gbm. The only difference is you need to incorporate the training data within the partial function.

```{r,eval=F}
pdp <- xgb.fit.final %>%
  partial(pred.var = "Gr_Liv_Area_clean", n.trees = 1576, grid.resolution = 100, train = features_train) %>%
  autoplot(rug = TRUE, train = features_train) +
  scale_y_continuous(labels = scales::dollar) +
  ggtitle("PDP")
```

```{r,eval=F}
ice <- xgb.fit.final %>%
  partial(pred.var = "Gr_Liv_Area_clean", n.trees = 1576, grid.resolution = 100, train = features_train, ice = TRUE) %>%
  autoplot(rug = TRUE, train = features_train, alpha = .1, center = TRUE) +
  scale_y_continuous(labels = scales::dollar) +
  ggtitle("ICE")
```

```{r,eval=F}
gridExtra::grid.arrange(pdp, ice, nrow = 1)
```
-->

## LIME

- LIME provides built-in functionality for xgboost objects (see ?model_type). 
- Just keep in mind that the local observations being analyzed need to be one-hot encoded in the same manner as we prepared the training and test data. Also, when you feed the training data into the lime::lime function be sure that you coerce it from a matrix to a data frame.

```{r}
# one-hot encode the local observations to be assessed.
local_obs_onehot <- vtreat::prepare(treatplan, local_obs, 
                                    varRestriction = new_vars)

# apply LIME
explainer <- lime(data.frame(features_train), xgb.fit.final)
explanation <- explain(local_obs_onehot, explainer, 
                       n_features = 5)
```

## Plot the features

```{r}
plot_features(explanation)
```


## Predicting on new observations

<!--
- Lastly, we predict on new observations; 
-->
unlike GBM we do not need to provide the number of trees. Our test set RMSE is only about $600 different than that produced by our gbm model.

```{r}
# predict values for test data
pred <- predict(xgb.fit.final, features_test)

# results
caret::RMSE(pred, response_test)
## [1] 21319.3
```

## Links and Resources - Boosting

### Links

- [**Gradient Boosting Machines**](http://uc-r.github.io/gbm_regression)

- [**How to Visualize Gradient Boosting Decision Trees With XGBoost in Python**](https://machinelearningmastery.com/visualize-gradient-boosting-decision-trees-xgboost-python/)

### Resources

- Geron (2017) - [**Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools and techniques to build intelligent systems**](http://shop.oreilly.com/product/0636920052289.do)


<!--
ToDo:

Have a look here:

https://bradleyboehmke.github.io/HOML/deep-learning.html
-->
