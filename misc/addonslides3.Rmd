---
title: "Untitled"
author: "Jan-Philipp Kolb"
date: "30 5 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## The plotted error rate

```{r,echo=F}
sqrtm1 <- sqrt(m1$mse[which.min(m1$mse)])
```


- The plotted error rate above is based on the OOB sample error and can be accessed directly at 

```{r,eval=F}
m1$mse
```

- The lowest error rate, is 344 trees providing an average home sales price error of  `r sqrtm1` Dollar.

```{r}
which.min(m1$mse)
sqrt(m1$mse[which.min(m1$mse)])
```


<!--
if we did not want to use the OOB samples. 
valid_split <- valid_split%>% na.omit()
-->

<!--
## A validation set to measure predictive accuracy

- `randomForest` also allows us to use a validation set to measure predictive accuracy 
- Here we split our training set further to create a training and validation set. 
- The validation data is in `xtest` and `ytest`.

```{r,eval=F}
set.seed(123)
valid_split <- initial_split(ames_train, .8)
# training data
ames_train_v2 <- analysis(valid_split)
# validation data
ames_valid <- rsample::assessment(valid_split)
x_test <- ames_valid[setdiff(names(ames_valid), "Sale_Price")]
y_test <- ames_valid$Sale_Price
```


## Run the model   

- With the command `randomForest`

```{r,eval=F}
rf_oob_comp <- randomForest(formula=Sale_Price ~ .,
  data=ames_train_v2,xtest = x_test,ytest=y_test)
```

```{r,eval=F,echo=F}
save(rf_oob_comp,file="../data/ml_rf_oob_comp.RData")
```

```{r,echo=F}
load("../data/ml_rf_oob_comp.RData")
```

### extract OOB & validation errors

```{r}
oob <- sqrt(rf_oob_comp$mse)
validation <- sqrt(rf_oob_comp$test$mse)
```



## compare error rates

- Extract OOB & validation errors

```{r,eval=F}
tibble::tibble(
  `Out of Bag Error` = oob,
  `Test error` = validation,
  ntrees = 1:rf_oob_comp$ntree
) %>%
  gather(Metric, RMSE, -ntrees) %>%
  ggplot(aes(ntrees, RMSE, color = Metric)) +
  geom_line() +
  scale_y_continuous(labels = scales::dollar) +
  xlab("Number of trees")
```

## 

```{r,echo=F}
## Compare error rates
tibble::tibble(
  `Out of Bag Error` = oob,
  `Test error` = validation,
  ntrees = 1:rf_oob_comp$ntree
) %>%
  gather(Metric, RMSE, -ntrees) %>%
  ggplot(aes(ntrees, RMSE, color = Metric)) +
  geom_line() +
  scale_y_continuous(labels = scales::dollar) +
  xlab("Number of trees")
```

-->


## Annex 


### Scoring models - metrics

- Many packages do not keep track of which observations were part of the OOB sample for a given tree and which were not. 
- If you are comparing multiple models, youâ€™d want to score each on the same validation set to compare performance. 
- It is possible to compute certain metrics such as [**root mean squared logarithmic error**](https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/loss-functions/mean-squared-logarithmic-error) (RMSLE) on the OOB sample, but it is not built in to all packages. 
- So if you want to compare multiple models or use a slightly less traditional loss function you will likely want to still perform cross validation.


<!--
https://maths-people.anu.edu.au/~johnm/courses/r/exercises/pdf/r-exercises.pdf
https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/tutorial-random-forest-parameter-tuning-r/tutorial/
-->

  


