---
title: "The dangers of machine learning"
author: "Alexander Murray-Watters"
date: "12 April 2019"
output: slidy_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Models are often uninterpretable
As the models produced by, e.g., a neural network are often
uninterpretable, it is difficult to work out where a model went wrong.

TODO: Insert bit on Google flu trends screw ups.
https://www.nature.com/news/when-google-got-flu-wrong-1.12413

## Biased data

Garbage in => Garbage out.

TODO: Insert bit on racist Google image search 

https://www.theguardian.com/technology/2018/jan/12/google-racism-ban-gorilla-black-people
https://www.theverge.com/2018/1/12/16882408/google-racist-gorillas-photo-recognition-algorithm-ai


## Example: How to make a racist AI


(Based on: https://blog.conceptnet.io/posts/2017/how-to-make-a-racist-ai-without-really-trying/)

We grab the GloVe pre-trained word vector first: https://nlp.stanford.edu/projects/glove/



A word vector is a representation of a word within what is called a
"word vector space". These vector spaces attempt to model the semantic
and syntatical context of words, and are usually constructed by
feeding a large body of text into a neural network. The neural network
is usually constructed so that given an input such as context (e.g.,
the other words in a sentence surounding the word to be predict),
predict what word should "fit". The other direction is also possible,
that is, predict "context" given a particular word.

This particular GloVe word vector dataset has a 1.9 million word
vocabulary is 1.75 Gigabytes large (compressed), 5 Gigabytes
uncompressed.

- Method for constructing dataset: https://nlp.stanford.edu/pubs/glove.pdf

- Dataset: http://nlp.stanford.edu/data/glove.42B.300d.zip




```{r}
library(dplyr)
library(glmnet)

glove.df <- read.csv("~/Downloads/glove.42B.300d.txt",
                 header=TRUE, sep=" ",quote="",
                 col.names=c("word",paste0("e",1:300)))

glove.df$word <- as.character(glove.df$word)


## Did the file read in correctly?
head(glove.df$word, n=100)

dim(glove.df)

head(glove.df$e1)

```

```{r}

# We use the scan function to read in the list of words as it has an
# easy way to distinguish comments from data (that way we don't have
# to bother with grep or regular expressions).
negative.words <- scan("../data/negative-words.txt", comment.char=";", what="", blank.lines.skip=T)

positive.words <- scan("../data/positive-words.txt", comment.char=";", what="", blank.lines.skip=T)

length(positive.words)
length(negative.words)

head(positive.words)
head(negative.words)


```

```{r}

## Locating postive and negative words in our dataset.
pos.vectors <- which(glove.df$word %in% positive.words)
neg.vectors <- which(glove.df$word %in% negative.words)

## Limiting dataset to only those words in our lists of pos/neg words.
glove.df <- glove.df[c(pos.vectors, neg.vectors),]

## Need to reassign posiions now (since we have a smaller dataset).
pos.vectors <- which(glove.df$word %in% positive.words)
neg.vectors <- which(glove.df$word %in% negative.words)


## Binary indicator for positive or negative words.
sentiment.vec <- ifelse(1:length(glove.df$word)%in%pos.vectors, 1, -1)

## backing up word vector
word.backup <- glove.df$word

## Converting to matrix for glmnet.
glove.df <- as.matrix(glove.df[,-1] )

rownames(glove.df) <- word.backup

## garbage collecting to recover RAM.
gc(); gc(); gc();


## Testing/traing datasets. 80% for training, 20% for testing.
train.df <- sample(1:length(word.backup), size=length(word.backup)*.8)

test.df <-  which(!(1:length(word.backup))%in%train.df)

## If this isn't 0 then we've messed up our spliting procedure.
sum(test.df%in%train.df)


## Fitting a model -- lasso regression.
fit.cv <- cv.glmnet(glove.df[train.df,], sentiment.vec[train.df],family="binomial" )

pred.fit <- predict(fit.cv$glmnet.fit, glove.df[test.df,], s=fit.cv$lambda.min)

pred.fit[sample(1:nrow(pred.fit), size=100),]



```


```{r}
library(ggplot2)
library(dplyr)

ggplot(tibble(pred.fit,
              pos.neg = as.character(sentiment.vec[c(pos.vectors, neg.vectors)[test.df]]))) +
  aes(x = pos.neg, y = pred.fit) +
  geom_boxplot() +
  labs(title = "Boxplot of model Coefficents by word type",
       x="Positive or Negative ", y = "Coefficent")


```


Even when a method appears to be unbiased on the surface, if there is
bias in how data are collected, the *model* will also be biased.

## Other examples

- The COMPAS algorithm (sometimes used to decide who gets parole),
resulted in [racist parole decisions](https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm)
	- Code and data used in the writer's analysis is available on [github](https://github.com/propublica/compas-analysis)

- If your model's predictions effect what new data is gathered (e.g.,
predictive policing) there is a risk of creating distorting feedback
loops. [One example](https://www.themarshallproject.org/2016/02/03/policing-the-future) involves using a model of where arrests for a
particular crime occurred to guide police patrol assignments, resulting
in more arrests in those areas (due to the increased patrol presence),
leading to more patrol assignments in that area, etc.
	- A notorious example of a feedback loop resulting in junk data
      and horrific consequences is the "[body
      count](https://en.wikipedia.org/wiki/Vietnam_War_body_count_controversy#Body_count_inflation)",
      which invovled using the number of "enemies" killed as a measure
      for how well the war was progressing. As commanders were assesed
      based on the their unit's count, there was a strong incentive
      for falsification -- in one way or another.

- Other concerning models include the evaluation of teachers,
  calculating credit scores, and citizenship scores (e.g., [social
  credit system in China](https://en.wikipedia.org/wiki/Social_Credit_System)).


## There is no magic to machine learning


- If someone can't explain how their method works in a simple way, but
insists it solves all of your problems, they're selling snake oil.

- The most oversold method right now (neural networks and deep
  learning), is essentially a form of non-linear regression. As with any
  form of regression, the model is only as good as its assumptions and
  data. 
	  - If you are trying to make predictions for data that is not
  within the range of your sample, the model will almost certainly perform poorly.
	  - If your data suffer from a selection effect, an incorrectly
        randomized experiment/trial, or other problems with the data
        gathering process, applying a machine learning model (like any
        statistical model) will perform poorly.

- Overfitting and out-of-sample prediction are still both
  problems. The only known solution (at the present time) is to
  already know a close to true underlying model (as in much of
  physics).

 "Once men turned their thinking over to machines in the hope that this
would set them free. But that only permitted other men with machines
to enslave them.” Frank Herbert, Dune, 1965.

# Further reading

 - Weapons of Math Destruction by Cathy O’Neil (non-technical, with more examples)
 
