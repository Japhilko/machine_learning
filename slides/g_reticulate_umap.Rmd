---
title: "Making Use of Machine Learning Resources from Python"
author: "Alexander Murray-Watters"
date: "April 9, 2019"
output: beamer_presentation
---


# Why Python? Isn't this an R course?

It'd be great if you only had to use a single programming language,
but unfortunately that's not the way the world works. While most new
statistical methods are initially implemented for R (as their creators
are statisticians), machine learning algorithms are usually created by
computer scientists, who have built their own data analysis tool set
around Python. While the algorithms are often ported to R (this is
especially easy when Python was simply used as a convenient "wrapper"
for the implementation), there is often a delay, and the quality of
the R implementation can be very poor.


# Installing Python

Go to https://www.python.org/ and download the latest version. Then
run the installer, choosing the defaults.

Install the `reticulate` package so that you can call python from R.

```{r, eval=FALSE}
##install.packages("reticulate")
```

Load the `reticulate` package.

```{r}

library(reticulate)
library(ggplot2)

```


# Calling python from R using reticulate

```{r}

## Importing the math library from python
math.main <- import("math")

## We access functions from the imported python library using the \$
## operator.

## Square root can take integers.
math.main$sqrt(4)

## Square root can take decimal numbers.
math.main$sqrt(4.2)

## Log is base e.
math.main$log(math.main$e)

## Base 10 log.
math.main$log10(10)

```




## The minimum you need to be aware of
Python, just like R, has different kinds of objects. Some commonly
used ones are:

1. Integers (1, 2, 3, 4,...)
2. Decimal (.34234, .123, pi)
   (Called "numeric" in R, "floats" in Python).
3. DataFrame (from the pandas library in R).
4. Array (called a matrix in R).
5. Series (called a vector in R).

There are many other kinds of objects (and you can create your
own). The relevant point for using reticulate is that you need to make
sure the kind of object you are using in R is being converted to the
right kind of object for the python function you are trying to make
use of.

If a function in python requires an integer, you can't simply pass it
a number from R, as R defaults to using "numeric" objects to represent
numbers. You have to convert the "numeric" to an "integer" first.

```{r}

a.number <- 1

class(a.number)

class(as.integer(a.number))

```



# An example using HDBSCAN

## The R version (from dbscan package)

```{r}

## cluster the moons data set with HDBSCAN

library(dbscan)

data(moons)
res <- dbscan::hdbscan(moons, minPts = 5)

res

plot(moons, col = res$cluster + 1, main="R implementation")



```

## Python version.

As the python version runs significantly faster (and uses less memory),
you may want to go to the hassle of using it instead of the R
implementation.

```{r}
library(reticulate)

hdbscan.main <- import("hdbscan")

hdbscan.py <- function(data, minPts){
    hdbscan.main$HDBSCAN(min_cluster_size=as.integer(minPts))$fit_predict(data)
}


clust.assignment <- hdbscan.py(data=as.matrix(moons), minPts=5)

plot(moons, col = clust.assignment + 1, main="Python implementation")


```



# An example using UMAP

Other algorithms are often too new to have mature R
implementations. One of the more exciting examples of this is a
non-linear dimension reduction technique called UMAP. While at least
one R version exists, it is significantly slower and more resource
intensive than the python implementation. The python version is also
constantly receiving updates, as it is the main focus of the
originator of the algorithm.

```{r}

umap.main <- import("umap")

## New version using reticulate.
umap <- function(x, n_neighbors = 10, min_dist = 0.1,
                 metric = 'euclidean', y = c(),
                 n_components = 2, ...){

    ## Have to convert in order to ensure reticulate gives 
    ## UMAP the right kind of python objects.
    data <- as.matrix(x)
    n_neighbors <- as.integer(n_neighbors)
    n_components <- as.integer(n_components)

    ## Intializing umap object with var. parameters.
    umap.obj <- umap.main$UMAP(n_neighbors = n_neighbors,
                               min_dist = min_dist,
                               metric = metric,
                               n_components = n_components,
                               ...)

    return(umap.obj$fit_transform(data, y = y))
}
 
## Generating fake standard normal data.

fake.data <- cbind(rnorm(n=100), rnorm(n=100), rnorm(n=100))

head(fake.data)

## Performing reduction.

dim.reduct <- umap(fake.data, n_neighbors = 15, min_dist = 0, metric = "euclidean", n_components = 1)

head(dim.reduct)

```

### A more interesting example involving cars

Using the built in `mtcars` dataset, we can see how the method performs (in contrast to say, PCA).

     A data frame with 32 observations on 11 (numeric) variables.

       1  mpg   Miles/(US) gallon
       2  cyl   Number of cylinders
       3  disp  Displacement (cu.in.)
       4  hp    Gross horsepower
       5  drat  Rear axle ratio
       6  wt    Weight (1000 lbs)
       7  qsec  1/4 mile time
       8  vs    Engine (0 = V-shaped, 1 = straight)
       9  am    Transmission (0 = automatic, 1 = manual)
       10  gear  Number of forward gears
       11  carb  Number of carburetors



```{r}

## Performing reduction.                                                                                                                                                                     
dim.reduct <- umap(mtcars[,-c(8,9)], n_neighbors = 15, min_dist = 0, metric = "euclidean", n_components = 2)
dim.reduct <-data.frame(dim.reduct, mtcars)

plot(prcomp(mtcars[,-c(8,9)], 2), main="PCA for comparison")

biplot(prcomp(mtcars[,-c(8,9)], 2), main="PCA for comparison")

ggplot(dim.reduct) +
  aes(x=X1, y=X2) +
  geom_point(aes(color=as.factor(gear))) +
  labs(title="", x="Dimension 1", y = "Dimension 2")

ggplot(dim.reduct) +
  aes(x=X1, y=X2) +
  geom_point(aes(color=as.factor(cyl))) +
  labs(title="", x="Dimension 1", y = "Dimension 2")

ggplot(dim.reduct) +
  aes(x=X1, y=X2) +
  geom_point(aes(color=as.factor(vs))) +
  labs(title="", x="Dimension 1", y = "Dimension 2")

ggplot(dim.reduct) +
  aes(x=X1, y=X2) +
  geom_point(aes(color=as.factor(am))) +
  labs(title="", x="Dimension 1", y = "Dimension 2")

ggplot(dim.reduct) +
  aes(x=X1, y=X2) +
  geom_point(aes(color=as.factor(carb))) +
  labs(title="", x="Dimension 1", y = "Dimension 2")

ggplot(dim.reduct) +
  aes(x=X1, y=X2) +
  geom_point(aes(color=disp)) +
  labs(title="", x="Dimension 1", y = "Dimension 2")

ggplot(dim.reduct) +
  aes(x=X1, y=X2) +
  geom_point(aes(color=hp)) +
  labs(title="", x="Dimension 1", y = "Dimension 2")

ggplot(dim.reduct) +
  aes(x=X1, y=X2) +
  geom_point(aes(color=drat)) +
  labs(title="", x="Dimension 1", y = "Dimension 2")

```

## Exercise: What happens if you extract more dimensions than original variables?
Does it run? What does the plotted output look like? 

## Exercise: Try changing the distance  metric  from 'euclidean'to something that doesn't make sense for this data. Do you still get an output?

## Exercise: What happens as you change the minimum number of neighbors? 


## Exercise Apply hdbscan to the output of UMAP. 


## Exercise: try changing the minimum distance between poins in the embeding. Does hdbscan's performance improve or declineas the distance increases/increases? 

## 


# Additional resources and further reading


- Automate the Boring Stuff with Python (applied intro to Python): https://automatetheboringstuff.com/

- Scikit-learn (general machine learning library): https://scikit-learn.org/stable/index.html

- Pandas (Python's version of data.frames and other R-like
  structures): https://pandas.pydata.org/
	  - 10 minute introduction: https://pandas.pydata.org/pandas-docs/stable/
 
- Keras (easy to use "deep learning" libray): https://keras.io/
	- Example workflow: https://pythonprogramming.net/introduction-deep-learning-python-tensorflow-keras/
	
- [R Interface to Python](https://rstudio.github.io/reticulate/)	

