---
title: "Supervised Learning - Trees, Bagging, Boosting"
author: "Jan-Philipp Kolb"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  beamer_presentation: 
    colortheme: rose
    fonttheme: structurebold
    highlight: pygments
    theme: Darmstadt
  slidy_presentation: 
    keep_md: yes
---

```{r setupbaggingboosting, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,message=F,warning=F)
library(knitr)
```


## [What is supervised learning?](https://elitedatascience.com/birds-eye-view)


<!--
https://lgatto.github.io/IntroMachineLearningWithR/supervised-learning.html#random-forest
-->


Supervised learning includes tasks for "labeled" data (i.e. you have a target variable).

- In practice, it's often used as an advanced form of predictive modeling.
 -  Each observation must be labeled with a "correct answer."
 -   Only then can you build a predictive model because you must tell the algorithm what's "correct" while training it (hence, "supervising" it).
 -   Regression is the task for modeling continuous target variables.
 -   Classification is the task for modeling categorical (a.k.a. "class") target variables.



<!--
- Example data are used to train a model.
- With this model the classification can be realized automatically. 


http://www.datenbanken-verstehen.de/lexikon/supervised-learning/

Daten einer Gruppierung zuzuordnen, die durch den Nutzenden vorgegeben sind, aber nicht jeder Datensatz manuell bewertet werden kann (z. B. Kreditbewilligung abhängig von Kredithöhe und Bonität). 

Die Aufgabe besteht darin, 

Ein Modell wird mit Beispieldaten aufgebaut, das die Zuordnung anschließend selbstständig übernimmt.
-->


## [Tree-Based Models](https://www.statmethods.net/advstats/cart.html)

- Trees are good for interpretation because they are simple

- Tree based methods involve stratifying or segmenting the predictor space
into a number of simple regions. ([Hastie and Tibshirani](https://lagunita.stanford.edu/c4x/HumanitiesScience/StatLearning/asset/trees.pdf))

- These methods do not deliver the best results concerning prediction accuracy. 

<!--
https://lagunita.stanford.edu/courses/HumanitiesSciences/StatLearning/Winter2016/about

https://lagunita.stanford.edu/c4x/HumanitiesScience/StatLearning/asset/trees.pdf
-->

<!--

https://en.wikipedia.org/wiki/Decision_tree_learning

https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf

https://www.analyticsvidhya.com/blog/2016/04/complete-tutorial-tree-based-modeling-scratch-in-python/
https://www.analyticsvidhya.com/blog/2016/02/complete-tutorial-learn-data-science-scratch/
-->


## Explanation: [**decision tree**](https://en.wikipedia.org/wiki/Decision_tree)

<!--
https://elitedatascience.com/algorithm-selection
-->

Decision trees model data as a "tree" of hierarchical branches. They make branches until they reach "leaves" that represent predictions.

![ [Decission tree on ElitedataScience](https://elitedatascience.com/algorithm-selection)](figure/Decision-Tree-Example.jpg)

## Summary decission trees

Due to their branching structure, decision trees can easily model nonlinear relationships.

- For example, let's say for Single Family homes, larger lots command higher prices.
 -  However, let's say for Apartments, smaller lots command higher prices (i.e. it's a proxy for urban / rural).
 -   This reversal of correlation is difficult for linear models to capture unless you explicitly add an interaction term (i.e. you can anticipate it ahead of time).
 -  On the other hand, decision trees can capture this relationship naturally.


```{r}
library(rpart)
```



<!--
https://www.guru99.com/r-decision-trees.html
-->

## [Classification Tree example](https://www.guru99.com/r-decision-trees.html)

The purpose of this dataset is to predict which people are more likely to survive after the collision with the iceberg. The dataset contains 13 variables and 1309 observations. The dataset is ordered by the variable X. 


```{r}
path <- 'https://raw.githubusercontent.com/thomaspernet/data_csv_r/master/data/titanic_csv.csv'
titanic <-read.csv(path)
shuffle_index <- sample(1:nrow(titanic))
kable(head(titanic))
```



<!--
## [**Decision trees used in data mining are of two main types:**](https://en.wikipedia.org/wiki/Decision_tree_learning)

- Classification tree

- Regression tree
-->

## [](https://elitedatascience.com/algorithm-selection)

Unfortunately, decision trees suffer from a major flaw as well. If you allow them to grow limitlessly, they can completely "memorize" the training data, just from creating more and more and more branches.

As a result, individual unconstrained decision trees are very prone to being overfit.




## Conditional inference tree

```{r}
library(party)
```

```{r,eval=F}
?ctree
```

- performs recursively univariate split recursively


- [**Vignette**](https://cran.r-project.org/web/packages/party/vignettes/party.pdf) package `party`


## [ctree example](https://datawookie.netlify.com/blog/2013/05/package-party-conditional-inference-trees/)

```{r,eval=F}
install.packages("party")
```

## The data behind

```{r}
airq <- subset(airquality, !is.na(Ozone))
summary(airq$Temp)
```

## A first model

```{r}
library(party)
```


```{r}
air.ct <- ctree(Ozone ~ ., data = airq, controls = ctree_control(maxsurrogate = 3))
```


## The plot for `ctree`

```{r}
plot(air.ct)
```




## Recursive partitioning algorithms are special cases of a
simple two-stage algorithm

- First partition the observations by univariate splits in a recursive way and 
- second fit a constant model in each cell of the resulting partition.


## [`ctree` - Regression](https://stats.stackexchange.com/questions/171301/interpreting-ctree-partykit-output-in-r)

```{r}
library(partykit)
```

```{r,eval=F}
?ctree
```

```{r}
airq <- subset(airquality, !is.na(Ozone))
airct <- ctree(Ozone ~ ., data = airq)
plot(airct, type = "simple")
```



## [Decision Trees](http://www.statmethods.net/advstats/cart.html)

[Regression tree vs. classification tree](http://www.statmethods.net/advstats/cart.html)


```{r}
library(rpart)
```

Grow a tree

```{r}
fit <- rpart(Kyphosis ~ Age + Number + Start,
   method="class", data=kyphosis)

printcp(fit) # display the results
plotcp(fit) # visualize cross-validation results
summary(fit) # detailed summary of splits
```

```{r}
# plot tree
plot(fit, uniform=TRUE,
   main="Classification Tree for Kyphosis")
text(fit, use.n=TRUE, all=TRUE, cex=.8)
```

[Decision Trees and Random Forest](https://cran.r-project.org/doc/contrib/Zhao_R_and_data_mining.pdf)


## [Ensembling](https://elitedatascience.com/overfitting-in-machine-learning)

Ensembles are machine learning methods for combining predictions from multiple separate models. 

<!--
There are a few different methods for ensembling, but the two most common are:
-->

### Bagging 

attempts to reduce the chance overfitting complex models.


- It trains a large number of "strong" learners in parallel.
-  A strong learner is a model that's relatively unconstrained.
-  Bagging then combines all the strong learners together in order to "smooth out" their predictions.

### Boosting 

attempts to improve the predictive flexibility of simple models.

-    It trains a large number of "weak" learners in sequence.
-    A weak learner is a constrained model (i.e. you could limit the max depth of each decision tree).
-    Each one in the sequence focuses on learning from the mistakes of the one before it.
-    Boosting then combines all the weak learners into a single strong learner.

### Bagging and boosting

While bagging and boosting are both ensemble methods, they approach the problem from opposite directions.

Bagging uses complex base models and tries to "smooth out" their predictions, while boosting uses simple base models and tries to "boost" their aggregate complexity.


## [Random Forest](https://www.datascience.com/resources/notebooks/random-forest-intro)

> Random forest aims to reduce the previously mentioned correlation issue by choosing only a subsample of the feature space at each split. Essentially, it aims to make the trees de-correlated and prune the trees by setting a stopping criteria for node splits, which I will cover in more detail later.

## [What are the advantages and disadvantages of decision trees?](https://elitedatascience.com/machine-learning-interview-questions-answers#supervised-learning)

Advantages: Decision trees are easy to interpret, nonparametric (which means they are robust to outliers), and there are relatively few parameters to tune.

Disadvantages: Decision trees are prone to be overfit. However, this can be addressed by ensemble methods like random forests or boosted trees.


## [Random forest](https://en.wikipedia.org/wiki/Random_forest)

- Ensemble learning method - multitude of decision trees 
- Random forests correct for decision trees' habit of overfitting to their training set.


![](figure/expl_rf.png)



## [Bagging](https://www.r-bloggers.com/improve-predictive-performance-in-r-with-bagging/)

- Bagging is also known as bootstrap aggregation.
- Bagging is a method for combining predictions from different regression or classification models and was developed by Leo Breiman.
- The results of the models are then averaged in the simplest case.
- The result of each model prediction is included in the prediction with the same weight.
- The weights could depend on the quality of the model prediction, i.e. "good" models are more important than "bad" models.
- Bagging leads to significantly improved predictions in the case of unstable models.

<!--
https://en.wikipedia.org/wiki/Bootstrap_aggregating
https://de.wikipedia.org/wiki/Bagging
http://topepo.github.io/caret/miscellaneous-model-functions.html#bagging-1
-->



## [Gradient boosting](https://en.wikipedia.org/wiki/Gradient_boosting)

Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. It builds the model in a stage-wise fashion like other boosting methods do, and it generalizes them by allowing optimization of an arbitrary differentiable loss function.

The idea of gradient boosting originated in the observation by Leo Breiman that boosting can be interpreted as an optimization algorithm on a suitable cost function.


Breiman, L. (1997). "Arcing The Edge". Technical Report 486. Statistics Department, University of California, Berkeley.


<!--
-->

## Explicit algorithms

Explicit regression gradient boosting algorithms were subsequently developed by Jerome H. Friedman,[2][3] simultaneously with the more general functional gradient boosting perspective of Llew Mason, Jonathan Baxter, Peter Bartlett and Marcus Frean.[4][5] 


The latter two papers introduced the view of boosting algorithms as iterative functional gradient descent algorithms. That is, algorithms that optimize a cost function over function space by iteratively choosing a function (weak hypothesis) that points in the negative gradient direction. This functional gradient view of boosting has led to the development of boosting algorithms in many areas of machine learning and statistics beyond regression and classification.


## [**Advantages of gradient boosting**](http://uc-r.github.io/gbm_regression)

- Often provides predictive accuracy that cannot be beat.
- Lots of flexibility - can optimize on different loss functions and provides several hyperparameter tuning options that make the function fit very flexible.
- No data pre-processing required - often works great with categorical and numerical values as is.
- Handles missing data - imputation not required.

## [**Disadvantages**](http://uc-r.github.io/gbm_regression) of gradient boosting


- GBMs will continue improving to minimize all errors. This can overemphasize outliers and cause overfitting. Must use cross-validation to neutralize.
- Computationally expensive - GBMs often require many trees (>1000) which can be time and memory exhaustive.
- The high flexibility results in many parameters that interact and influence heavily the behavior of the approach (number of iterations, tree depth, regularization parameters, etc.). This requires a large grid search during tuning.
- Less interpretable although this is easily addressed with various tools (variable importance, partial dependence plots, LIME, etc.).


## Two types of errors for tree methods

### Bias related errors

- Adaptive boosting
- Gradient boosting

### Variance related errors

- Bagging
- Random forest

<!--
https://www.slideshare.net/JaroslawSzymczak1/gradient-boosting-in-practice-a-deep-dive-into-xgboost

What if we, instead of reweighting examples, made some corrections to prediction errors directly?

Residual is a gradient of single observation error contribution in one of the most common evaluation measure for regression: RMSE
-->



## [Gradient Boosting for Linear Regression - why does it not work?](https://stats.stackexchange.com/questions/186966/gradient-boosting-for-linear-regression-why-does-it-not-work)

While learning about Gradient Boosting, I haven't heard about any constraints regarding the properties of a "weak classifier" that the method uses to build and ensemble model. However, I could not imagine an application of a GB that uses linear regression, and in fact when I've performed some tests - it doesn't work. I was testing the most standard approach with a gradient of sum of squared residuals and adding the subsequent models together.

The obvious problem is that the residuals from the first model are populated in such manner that there is really no regression line to fit anymore. My another observation is that a sum of subsequent linear regression models can be represented as a single regression model as well (adding all intercepts and corresponding coefficients) so I cannot imagine how that could ever improve the model. The last observation is that a linear regression (the most typical approach) is using sum of squared residuals as a loss function - the same one that GB is using.

I also thought about lowering the learning rate or using only a subset of predictors for each iteration, but that could still be summed up to a single model representation eventually, so I guess it would bring no improvement.

What am I missing here? Is linear regression somehow inappropriate to use with Gradient Boosting? Is it because the linear regression uses the sum of squared residuals as a loss function? Are there any particular constraints on the weak predictors so they can be applied to Gradient Boosting?



## Links - Boosting

- [**Gradient Boosting Machines**](http://uc-r.github.io/gbm_regression)


- [How to Visualize Gradient Boosting Decision Trees With XGBoost in Python](https://machinelearningmastery.com/visualize-gradient-boosting-decision-trees-xgboost-python/)


<!--
https://www.researchgate.net/figure/A-simple-example-of-visualizing-gradient-boosting_fig5_326379229
-->


## Links

- [**Vignette**](https://cran.r-project.org/web/packages/partykit/vignettes/ctree.pdf) for package `partykit` 

- [Conditional Inference Trees](https://rpubs.com/awanindra01/ctree)


- [Conditional inference trees vs traditional decision trees](https://stats.stackexchange.com/questions/12140/conditional-inference-trees-vs-traditional-decision-trees)

- [Video on tree based methods](https://www.youtube.com/watch?v=6ENTbK3yQUQ)




